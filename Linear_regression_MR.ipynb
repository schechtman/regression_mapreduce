{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "article : https://arxiv.org/pdf/1307.0048.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée du projet est d'implementer un algorithme mapreduce sur la régression linéaire pénalisée, quand $X \\in \\mathbb{R}^{n \\times p}$ avec $p << n$.\n",
    "\n",
    "Cela correspond à un type de problème ou le nombre de features $p$ (les caractéristiques d'un individu ou un produit) est assez petit et il est envisageable de les stocker en mémoire, alors que la taille du dataset $n$ et très grande et on voudrait faire du calcul distributé dessus\n",
    "\n",
    "L'idée de l'agorithme est alors d'exprimer la quantité à minimiser en fonction des matrices ou des vecteurs dont la dimension est une fonction de $p$ ($p\\times p$ ou $p$ en fait). Et calculer ces quantitées à partir de $X \\in \\mathbb{R}^{n \\times p}$ en faisant une reduction sur $n$ (qui est la taille de notre dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1\n",
    "\n",
    "En notant $X_c \\in \\mathbb{R}^{n \\times p }$ la matrice centrée réduite de $X$. On a :\n",
    "$$X_c = (X - \\mathbb{1} (\\bar{X_1}, \\dots, \\bar{X_p}))D^{-1}$$\n",
    "\n",
    "et\n",
    "$$\\begin{align}\n",
    "& & \\ ||Y - \\alpha \\mathbb{1} - X \\beta||_2^2 + p_\\lambda(\\beta) \\\\\n",
    "= & \\ ||Y - \\alpha \\mathbb{1} - (X_cD + \\mathbb{1} (\\bar{X_1}, \\dots, \\bar{X_p})) \\beta||_2 + p_\\lambda(\\beta) \\\\\n",
    "= & \\ ||Y - (\\alpha + (\\bar{X_1}, \\dots, \\bar{X_p})) \\beta) \\mathbb{1} - X_c D \\beta ||_2 + p_\\lambda(\\beta)\n",
    "\\end{align}$$\n",
    "\n",
    "On a donc que minimiser:\n",
    "$$||Y - \\alpha \\mathbb{1} - X \\beta||_2^2 + p_\\lambda(\\beta) $$\n",
    "revient à minimiser:\n",
    "\n",
    "$$\\begin{align}\n",
    "||Y - \\hat{\\alpha} \\mathbb{1} - X_c \\hat{\\beta}||_2^2 + p_\\lambda(\\hat{\\beta})\n",
    "\\end{align}$$\n",
    "Avec le changement de variable:\n",
    "\n",
    "$$\\begin{align} \n",
    "\\hat{\\alpha}&= \\alpha + \\left(\\bar{X_1}, \\dots, \\bar{X_p}\\right) \\beta \\\\\n",
    "\\hat{\\beta} &= D \\beta\n",
    "\\end{align}\n",
    " $$\n",
    " \n",
    " avec D la matrice diagonale des déviations standards.\n",
    " \n",
    " Si on developpe la dernière équation on trouve:\n",
    " $$\\begin{align}\n",
    "\\ &||Y - \\hat{\\alpha} \\mathbb{1} - X_c \\hat{\\beta}||_2^2 + p_\\lambda(\\hat{\\beta}) \\\\\n",
    "= \\ & Y^TY  + n \\alpha^2 + \\hat{\\beta}^TX_c^TX_c\\beta - 2 n \\alpha \\bar{Y} + 2 \\alpha \\mathbb{1}^X_c\\beta - 2Y^TX_c\\hat{\\beta} + p_\\lambda(\\hat{\\beta})\\\\\n",
    "\\end{align}$$\n",
    " \n",
    " Comme $X_c$ est centrée  le minimum est atteint pour $\\hat{\\alpha} = \\bar{Y}$ et $\\alpha \\mathbb{1}^X_c = 0$.\n",
    " \n",
    " L'expression à minimiser devient donc:\n",
    " \n",
    " $$\\begin{align}\n",
    "  &\\ \\hat{\\beta} = \\arg \\min_{\\beta}  Y^TY - n \\bar{Y}^2 + \\beta^TX_c^TX_c\\beta - 2Y^TX_c\\beta + p_\\lambda(\\beta) \\\\\n",
    "   \\Leftrightarrow &\\ \\hat{\\beta} =  \\arg \\min_{\\beta} \\beta^TX_c^TX_c\\beta - 2Y^TX_c\\beta + p_\\lambda(\\beta) \\\\\n",
    "   \\Leftrightarrow &\\ \\hat{\\beta} =  \\arg \\min_{\\beta} \\beta^TD^{-1}(X^TX - n(\\bar{X_1}, \\dots, \\bar{X_p})^T (\\bar{X_1}, \\dots, \\bar{X_p}))D^{-1}\\beta - 2(Y^TX - n \\bar{Y}(\\bar{X_1}, \\dots, \\bar{X_p}))D^{-1}\\beta + p_\\lambda(\\beta)\n",
    " \\end{align}\n",
    " $$\n",
    " \n",
    " $X^TX, Y^TX, D^{-1}, (\\bar{X_1}, \\dots, \\bar{X_p})^T (\\bar{X_1}, \\dots, \\bar{X_p})$ sont des matrices de taille $p \\times p$. On peut par hypothèse les stocker en mémoire et résoudre ce problème par une des méthodes d'optimisation classiques (coordinate descent par exemple).\n",
    " \n",
    "Lors de l'étape map-reduce on calculera $X^TX, Y^TX, \\bar{Y}, (\\bar{X_1}, \\dots, \\bar{X_p}), COV(X)$ à partir desquelles on retrouvera les quantités citées précédemment.\n",
    "\n",
    "\n",
    "# Rajouter le fait qu'on fait de la cross validation en même temps (en faisant un reducebyKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize \n",
    "def PenalizedLR_MR(Data, k, lambdas, penalizer=\"ridge\"):\n",
    "    \"\"\"\n",
    "    Data: an RDD each rows of which is a tuple (x, y)\n",
    "    k: number of partitions for splitting (k>2 for implementation reasons)\n",
    "    lambdas: list of lambdas to test on\n",
    "    penalizer: penalization term (only \"ridge\" is avaialble for now)\n",
    "    \n",
    "    \"\"\"\n",
    "    def map_statistics(row):\n",
    "        # calculate statistics for one row [size, mean(x), mean(y), Y^TY, y * x, cov(x)]\n",
    "        x = row[0]\n",
    "        y = row[1]\n",
    "        return (np.random.randint(k), [1, x, y, y**2, y * x, np.zeros((len(row[0]), len(row[0])))])\n",
    "\n",
    "\n",
    "    def reduce_statistics(row1, row2):\n",
    "        #combined with map_statistics returns [size, mean(X), mean(Y), Y^TY, Y^TX, Cov(X)]\n",
    "        s_1 = row1[0]\n",
    "        s_2 = row2[0]\n",
    "        mean_x = s_1 / (s_1 + s_2) * row1[1] + s_2 / (s_1 + s_2) * row2[1]\n",
    "        mean_y = s_1 / (s_1 + s_2) * row1[2] + s_2 / (s_1 + s_2) * row2[2]\n",
    "        \n",
    "        mean_substraction = (row1[1] - row2[1]).reshape((1, -1))\n",
    "        cov = s_1 / (s_1 + s_2) * row1[5] + s_2 / (s_1 + s_2) * row2[5] + s_1 * s_2 / (\n",
    "            s_1 + s_2)**2 * (mean_substraction).T.dot(mean_substraction)\n",
    "        emit = [s_1 + s_2, mean_x, mean_y, row1[3] +\n",
    "                row2[3], row1[4] + row2[4], cov]\n",
    "        return emit\n",
    "\n",
    "    statistics = Data.map(map_statistics)\n",
    "    statistics = statistics.reduceByKey(reduce_statistics)\n",
    "\n",
    "    # Cross validation\n",
    "    test_errors = []\n",
    "    for lmbda in lambdas:\n",
    "        error = 0\n",
    "        for i in range(k):\n",
    "            #do the split\n",
    "            statistics_train = statistics.filter(lambda row: row[0] != i)\n",
    "            \n",
    "            statistics_test = statistics.filter(lambda row: row[0] == i).collect()[0][1]\n",
    "            #calculate the needed dot products for ou train dataset\n",
    "            size_train, means_X_train, mean_Y_train, YT_Y_train, YT_X_train, COV_X_train  = statistics_train.map(lambda row: row[1]).reduce(reduce_statistics)\n",
    "            #reshape our vectors\n",
    "            means_X_train = means_X_train.reshape(-1, 1)\n",
    "            YT_X_train = YT_X_train.reshape(-1, 1)\n",
    "\n",
    "            p = COV_X_train.shape[0]\n",
    "            XT_X_train = size_train * (COV_X_train + means_X_train.dot(means_X_train.T))\n",
    "            D_inv_train = np.diag([1 / np.sqrt(COV_X_train[i,i]) for i in range(p)])\n",
    "            D_train = np.diag([np.sqrt(COV_X_train[i,i]) for i in range(p)])\n",
    "            \n",
    "            #calculate statistics foro our test dataset:\n",
    "            size_test = statistics_test[0]\n",
    "            means_X_test = statistics_test[1].reshape(-1, 1)\n",
    "            mean_Y_test = statistics_test[2]\n",
    "            YT_Y_test = statistics_test[3]\n",
    "            YT_X_test = statistics_test[4].reshape(-1, 1)\n",
    "            COV_X_test = statistics_test[5]\n",
    "            \n",
    "            \n",
    "            XT_X_test = size_test * (COV_X_test + means_X_test.T.dot(means_X_test))\n",
    "            D_inv_test = np.diag([1 / np.sqrt(COV_X_test[i,i]) for i in range(p)])\n",
    "            D_test = np.diag([np.sqrt(COV_X_train[i,i]) for i in range(p)])\n",
    "            \n",
    "            def beta_objective(beta):\n",
    "                pen_term = 0\n",
    "                if penalizer == \"Ridge\":\n",
    "                    pen_term = lmbda + np.linalg.norm(beta**2)\n",
    "                #the simplified objective function for beta\n",
    "                linear_term = -(YT_X_train - size_train * mean_Y_train * means_X_train).T.dot(D_inv_train).dot(beta)\n",
    "                quadratic_term = 1 /2 * beta.dot(D_inv_train).dot(XT_X_train - size_train * means_X_train.dot(means_X_train.T)).dot(D_inv_train).dot(beta)\n",
    "                return linear_term + quadratic_term + pen_term\n",
    "            \n",
    "            \n",
    "            alpha_hat = mean_Y_test\n",
    "            beta_hat = minimize(beta_objective, np.zeros(p), method=\"CG\").x\n",
    "            \n",
    "            def test_error(alpha, beta):\n",
    "                quadratic_term = YT_Y_test - size_test * alpha**2 + beta.dot(D_inv_test).dot(XT_X_test - size_test * means_X_test.T.dot(means_X_test)).dot(D_inv_test).dot(beta)\n",
    "                double_term = -2 * alpha * mean_Y_test -2 * (YT_X_test - size_test * mean_Y_test * means_X_test).T.dot(D_inv_test).dot(beta)\n",
    "                return quadratic_term + double_term\n",
    "            error += test_error(alpha_hat, beta_hat)\n",
    "        test_errors.append(error)\n",
    "    \n",
    "    #choose best lambda:\n",
    "    best_i = np.argmin(test_errors)\n",
    "    best_lambda = lambdas[best_i]\n",
    "\n",
    "    #calculate the needed dot products\n",
    "    size, means_X, mean_Y, YT_Y, YT_X, COV_X  = statistics.map(lambda x : x[1]).reduce(reduce_statistics)\n",
    "    \n",
    "    #reshape our vectors\n",
    "    means_X = means_X.reshape(-1, 1)\n",
    "    YT_X = YT_X.reshape(-1, 1)\n",
    "    \n",
    "    p = COV_X.shape[0]\n",
    "    XT_X = size * (COV_X + means_X.dot(means_X.T))\n",
    "    D_inv = np.diag([1 / np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "    D = np.diag([np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "    \n",
    "    def beta_objective(beta):\n",
    "        pen_term = 0\n",
    "        if penalizer == \"Ridge\":\n",
    "            pen_term = lmbda + np.linalg.norm(beta**2)\n",
    "        #changing variables\n",
    "        beta = D.dot(beta)\n",
    "        #the simplified objective function for beta\n",
    "        linear_term = -(YT_X - size * mean_Y * means_X).T.dot(D_inv).dot(beta)\n",
    "        quadratic_term = 1 /2 * beta.dot(D_inv).dot(XT_X - size * means_X.dot(means_X.T)).dot(D_inv).dot(beta)\n",
    "        return linear_term + quadratic_term + pen_term\n",
    "    \n",
    "    #calculate final alpha, beta\n",
    "    beta = minimize(beta_objective, np.zeros(p), method=\"CG\").x\n",
    "    alpha_hat = mean_Y\n",
    "    alpha = alpha_hat - means_X.T.dot(beta)\n",
    "    \n",
    "    return (alpha, beta, best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques tests\n",
    "\n",
    "n = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 100\n",
    "n = 500\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "\n",
    "\n",
    "idx = np.arange(p)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[40:] = 0.\n",
    "    \n",
    "y = X.dot(coefs)\n",
    "\n",
    "Data_array = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data_rdd = sc.parallelize(Data_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.81162156e-05] 0.004597685090530298 0.01\n"
     ]
    }
   ],
   "source": [
    "alpha, beta, lmbda = PenalizedLR_MR(Data=Data_rdd, k=5, lambdas=np.logspace(-2, 2, 5), penalizer=\"Ridge\")\n",
    "\n",
    "print(alpha, np.linalg.norm(beta - coefs, ord=2), lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008422742420145918 0.32487655733699944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = []\n",
    "lambdas = np.logspace(-2, 2, 5)\n",
    "for lmbda in lambdas:\n",
    "    clf = Ridge(lmbda, fit_intercept=True)\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "best_i = np.argmin(scores)\n",
    "best_lambda = lambdas[best_i]\n",
    "clf = Ridge(best_lambda, fit_intercept=True)\n",
    "clf.fit(X, y)\n",
    "print(clf.intercept_, np.linalg.norm(clf.coef_ - coefs, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 14.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit PenalizedLR_MR(Data=Data_rdd, k=5, lambdas=np.logspace(-2, 2, 5), penalizer=\"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 63.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "scores = []\n",
    "lambdas = np.logspace(-2, 2, 5)\n",
    "for lmbda in lambdas:\n",
    "    clf = Ridge(lmbda, fit_intercept=True)\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "best_i = np.argmin(scores)\n",
    "best_lambda = lambdas[best_i]\n",
    "clf = Ridge(best_lambda, fit_intercept=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$n$ = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 100\n",
    "n = 10000\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "\n",
    "\n",
    "idx = np.arange(p)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[40:] = 0.\n",
    "    \n",
    "y = X.dot(coefs)\n",
    "\n",
    "Data_array = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data_rdd = sc.parallelize(Data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 20.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit PenalizedLR_MR(Data=Data_rdd, k=5, lambdas=np.logspace(-2, 2, 5), penalizer=\"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 603 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "scores = []\n",
    "lambdas = np.logspace(-2, 2, 5)\n",
    "for lmbda in lambdas:\n",
    "    clf = Ridge(lmbda, fit_intercept=True)\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "best_i = np.argmin(scores)\n",
    "best_lambda = lambdas[best_i]\n",
    "clf = Ridge(best_lambda, fit_intercept=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n$ = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 100\n",
    "n = 100000\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "\n",
    "\n",
    "idx = np.arange(p)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[40:] = 0.\n",
    "    \n",
    "y = X.dot(coefs)\n",
    "\n",
    "Data_array = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data_rdd = sc.parallelize(Data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 24.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit PenalizedLR_MR(Data=Data_rdd, k=5, lambdas=np.logspace(-2, 2, 5), penalizer=\"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 5.54 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "scores = []\n",
    "lambdas = np.logspace(-2, 2, 5)\n",
    "for lmbda in lambdas:\n",
    "    clf = Ridge(lmbda, fit_intercept=True)\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "best_i = np.argmin(scores)\n",
    "best_lambda = lambdas[best_i]\n",
    "clf = Ridge(best_lambda, fit_intercept=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que pour $n$ petit, la regression Ridge de scikit-leran est beaucoup plus rapide que notre implémentation, ce qui est normal vu qu'elle est spécialement optimisée pour python (probablement codée en cython même).\n",
    "\n",
    "Cependant quand $n$ augmente l'écart diminue sensiblement passant de au moins 20 fois pour $n=500$ à moins de 5 fois pour $n=100000$. Ce qui laisse supposer un certain avantage de notre méthode si elle était executée sur un grand nombre de machine et son optimisation serait plus optimisée"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
