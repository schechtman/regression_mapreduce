{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eléments logiciels pour le traitement des données massives (ENSAE 3A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sholom Schechtman et Nicolas Schreuder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article : https://arxiv.org/pdf/1307.0048.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée du projet est d'implémenter un algorithme MapReduce pour le problème de régression linéaire pénalisée, quand la matrice de features $X \\in \\mathbb{R}^{n \\times p}$ avec $p << n$.\n",
    "\n",
    "Cela correspond à un type de problème ou le nombre de features $p$ (les caractéristiques d'un individu ou d'un produit) est assez petit et il est envisageable de les stocker en mémoire, alors que la taille du dataset $n$ et très grande, ce qui nous pousse à passer par une étape MapReduce.\n",
    "\n",
    "L'idée de l'agorithme est alors d'exprimer la quantité à minimiser en fonction des matrices ou des vecteurs dont la dimension est une fonction de $p$ ($p\\times p$ ou $p$) mais pas de $n$. Puis de calculer ces quantitées à partir de $X \\in \\mathbb{R}^{n \\times p}$ en faisant une reduction sur $n$ (qui est la taille de notre dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme 1\n",
    "\n",
    "Le problème de régression linéaire pénalisée et de la forme : \n",
    "\n",
    "$$\\min_\\beta ||Y - \\alpha \\mathbb{1} - X \\beta||_2^2 + p_\\lambda(\\beta) $$\n",
    "\n",
    "avec $Y$ le vecteur d'étiquettes, $\\alpha$ un biais, $X$ la matrice de features, $\\beta$ le vecteur des coefficients (à estimer) et une fonction de pénalisation $p_\\lambda$ (par exemple la norme euclidienne pour la régression Ridge ou la norme $l_1$ pour le Lasso).\n",
    "\n",
    "En notant $X_c \\in \\mathbb{R}^{n \\times p }$ la matrice centrée réduite de $X$. Nous aavons :\n",
    "$$X_c = (X - \\mathbb{1} (\\bar{X_1}, \\dots, \\bar{X_p}))D^{-1}$$\n",
    "\n",
    "avec $D$ la matrice diagonale des écarts types.\n",
    "\n",
    "Ainsi nous pouvons réécrire la fonction objectif :\n",
    "\n",
    "$$\\begin{align} ||Y - \\alpha \\mathbb{1} - X \\beta||_2^2 + p_\\lambda(\\beta)\n",
    "= & \\ ||Y - \\alpha \\mathbb{1} - (X_cD + \\mathbb{1} (\\bar{X_1}, \\dots, \\bar{X_p})) \\beta||_2 + p_\\lambda(\\beta) \\\\\n",
    "= & \\ ||Y - (\\alpha + (\\bar{X_1}, \\dots, \\bar{X_p})) \\beta) \\mathbb{1} - X_c D \\beta ||_2 + p_\\lambda(\\beta)\n",
    "\\end{align}$$\n",
    "\n",
    "Nous avons donc que minimiser en $(\\alpha, \\beta)$ :\n",
    "\n",
    "$$||Y - \\alpha \\mathbb{1} - X \\beta||_2^2 + p_\\lambda(\\beta) $$\n",
    "\n",
    "revient à minimiser en $(\\hat{\\alpha}, \\hat{\\beta})$:\n",
    "\n",
    "$$\\begin{align}\n",
    "||Y - \\hat{\\alpha} \\mathbb{1} - X_c \\hat{\\beta}||_2^2 + p_\\lambda(\\hat{\\beta})\n",
    "\\end{align}$$\n",
    "\n",
    "Avec le changement de variable:\n",
    "\n",
    "$$\\begin{align} \n",
    "\\hat{\\alpha}&= \\alpha + \\left(\\bar{X_1}, \\dots, \\bar{X_p}\\right) \\beta \\\\\n",
    "\\hat{\\beta} &= D \\beta\n",
    "\\end{align}\n",
    " $$\n",
    "\n",
    " \n",
    "En développant la dernière équation nous obtenons :\n",
    "\n",
    " $$\\begin{align}\n",
    "||Y - \\hat{\\alpha} \\mathbb{1} - X_c \\hat{\\beta}||_2^2 + p_\\lambda(\\hat{\\beta}) = \\ & Y^TY  + n \\alpha^2 + \\hat{\\beta}^TX_c^TX_c\\beta - 2 n \\alpha \\bar{Y} + 2 \\alpha \\mathbb{1}^X_c\\beta - 2Y^TX_c\\hat{\\beta} + p_\\lambda(\\hat{\\beta})\\\\\n",
    "\\end{align}$$\n",
    " \n",
    " Comme $X_c$ est centrée  le minimum est atteint pour $\\hat{\\alpha} = \\bar{Y}$ et $\\alpha \\mathbb{1}^X_c = 0$.\n",
    " \n",
    " L'expression à minimiser devient finalement :\n",
    " \n",
    " $$\\begin{align}\n",
    "  &\\ \\hat{\\beta} = \\arg \\min_{\\beta}  Y^TY - n \\bar{Y}^2 + \\beta^TX_c^TX_c\\beta - 2Y^TX_c\\beta + p_\\lambda(\\beta) \\\\\n",
    "   \\Leftrightarrow &\\ \\hat{\\beta} =  \\arg \\min_{\\beta} \\beta^TX_c^TX_c\\beta - 2Y^TX_c\\beta + p_\\lambda(\\beta) \\\\\n",
    "   \\Leftrightarrow &\\ \\hat{\\beta} =  \\arg \\min_{\\beta} \\beta^TD^{-1}(X^TX - n(\\bar{X_1}, \\dots, \\bar{X_p})^T (\\bar{X_1}, \\dots, \\bar{X_p}))D^{-1}\\beta - 2(Y^TX - n \\bar{Y}(\\bar{X_1}, \\dots, \\bar{X_p}))D^{-1}\\beta + p_\\lambda(\\beta)\n",
    " \\end{align}\n",
    " $$\n",
    " \n",
    " $X^TX, Y^TX, D^{-1}, (\\bar{X_1}, \\dots, \\bar{X_p})^T (\\bar{X_1}, \\dots, \\bar{X_p})$ sont des matrices de taille $p \\times p$. On peutn par hypothèse, les stocker en mémoire et résoudre ce problème par une des méthodes d'optimisation classiques (coordinate descent par exemple).\n",
    " \n",
    "Lors de l'étape map-reduce on calculera $X^TX, Y^TX, \\bar{Y}, (\\bar{X_1}, \\dots, \\bar{X_p}), COV(X)$ à partir desquelles on retrouvera les quantités citées précédemment.\n",
    "\n",
    "Il faut en général fixer un hyper-paramètre, $\\lambda$, régissant l'impact de la pénalisation sur la fonction de coût. \n",
    "Ce paramètre est le plus souvent fixé par validation croisée. \n",
    "Nous incorporons cette validation croisée dans notre implémentation en partitionant l'ensemble des observations en $K$ classes (de façon aléatoire) grâce à la fonction $\\textit{reducebyKey}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize \n",
    "def PenalizedLR_MR(Data, k, lambdas, penalizer=\"ridge\"):\n",
    "    \"\"\"\n",
    "    Data: a RDD, each rows is a tuple (x, y)\n",
    "    k: number of partitions for splitting (k needs to be strictly greater than 3)\n",
    "    lambdas: list of lambdas (penalization parameter) to compare by CV \n",
    "    penalizer: penalization term (only \"ridge\" is available for now)\n",
    "    \"\"\"\n",
    "    \n",
    "    def map_statistics(row):\n",
    "        # Compate statistics for one row [size, mean(x), mean(y), Y^TY, y * x, cov(x)]\n",
    "        x = row[0]\n",
    "        y = row[1]\n",
    "        return (np.random.randint(k), [1, x, y, y**2, y * x, np.zeros((len(row[0]), len(row[0])))])\n",
    "\n",
    "\n",
    "    def reduce_statistics(row1, row2):\n",
    "        # Combined with map_statistics, returns [size, mean(X), mean(Y), Y^TY, Y^TX, Cov(X)]\n",
    "        s_1 = row1[0]\n",
    "        s_2 = row2[0]\n",
    "        mean_x = s_1 / (s_1 + s_2) * row1[1] + s_2 / (s_1 + s_2) * row2[1]\n",
    "        mean_y = s_1 / (s_1 + s_2) * row1[2] + s_2 / (s_1 + s_2) * row2[2]\n",
    "        \n",
    "        mean_substraction = (row1[1] - row2[1]).reshape((1, -1))\n",
    "        cov = s_1 / (s_1 + s_2) * row1[5] + s_2 / (s_1 + s_2) * row2[5] + s_1 * s_2 / (\n",
    "            s_1 + s_2)**2 * (mean_substraction).T.dot(mean_substraction)\n",
    "        emit = [s_1 + s_2, mean_x, mean_y, row1[3] +\n",
    "                row2[3], row1[4] + row2[4], cov]\n",
    "        return emit\n",
    "\n",
    "    statistics = Data.map(map_statistics)\n",
    "    statistics = statistics.reduceByKey(reduce_statistics)\n",
    "\n",
    "    # Cross validation\n",
    "    test_errors = []\n",
    "    for lmbda in lambdas:\n",
    "        error = 0\n",
    "        for i in range(k):\n",
    "            # Split in train/test\n",
    "            statistics_train = statistics.filter(lambda row: row[0] != i)\n",
    "            statistics_test = statistics.filter(lambda row: row[0] == i).collect()[0][1]\n",
    "            \n",
    "            # Compute the dot products we need for our train dataset\n",
    "            size_train, means_X_train, mean_Y_train, YT_Y_train, YT_X_train, COV_X_train  = statistics_train.map(lambda row: row[1]).reduce(reduce_statistics)\n",
    "            \n",
    "            # Reshape our vectors\n",
    "            means_X_train = means_X_train.reshape(-1, 1)\n",
    "            YT_X_train = YT_X_train.reshape(-1, 1)\n",
    "\n",
    "            p = COV_X_train.shape[0]\n",
    "            XT_X_train = size_train * (COV_X_train + means_X_train.dot(means_X_train.T))\n",
    "            D_inv_train = np.diag([1 / np.sqrt(COV_X_train[i,i]) for i in range(p)])\n",
    "            D_train = np.diag([np.sqrt(COV_X_train[i,i]) for i in range(p)])\n",
    "            \n",
    "            # Compute statistics for our test dataset:\n",
    "            size_test = statistics_test[0]\n",
    "            means_X_test = statistics_test[1].reshape(-1, 1)\n",
    "            mean_Y_test = statistics_test[2]\n",
    "            YT_Y_test = statistics_test[3]\n",
    "            YT_X_test = statistics_test[4].reshape(-1, 1)\n",
    "            COV_X_test = statistics_test[5]\n",
    "            \n",
    "            XT_X_test = size_test * (COV_X_test + means_X_test.T.dot(means_X_test))\n",
    "            D_inv_test = np.diag([1 / np.sqrt(COV_X_test[i,i]) for i in range(p)])\n",
    "            D_test = np.diag([np.sqrt(COV_X_train[i,i]) for i in range(p)])\n",
    "            \n",
    "            def beta_objective(beta):\n",
    "                pen_term = 0\n",
    "                if penalizer == \"Ridge\":\n",
    "                    pen_term = lmbda + np.linalg.norm(beta**2)\n",
    "                    \n",
    "                # Define the simplified objective function for beta\n",
    "                linear_term = -(YT_X_train - size_train * mean_Y_train * means_X_train).T.dot(D_inv_train).dot(beta)\n",
    "                quadratic_term = (1 /2 * beta.dot(D_inv_train).dot(XT_X_train - size_train * \n",
    "                                                                   means_X_train.dot(means_X_train.T)).dot(D_inv_train).dot(beta))\n",
    "                return linear_term + quadratic_term + pen_term\n",
    "            \n",
    "            \n",
    "            alpha_hat = mean_Y_test\n",
    "            beta_hat = minimize(beta_objective, np.zeros(p), method=\"CG\").x\n",
    "            \n",
    "            def test_error(alpha, beta):\n",
    "                quadratic_term = YT_Y_test - size_test * alpha**2 + beta.dot(D_inv_test).dot(XT_X_test - size_test * means_X_test.T.dot(means_X_test)).dot(D_inv_test).dot(beta)\n",
    "                double_term = -2 * alpha * mean_Y_test -2 * (YT_X_test - size_test * mean_Y_test * means_X_test).T.dot(D_inv_test).dot(beta)\n",
    "                return quadratic_term + double_term\n",
    "            \n",
    "            error += test_error(alpha_hat, beta_hat)\n",
    "        test_errors.append(error)\n",
    "    \n",
    "    # Choose best lambda according to mean error\n",
    "    best_i = np.argmin(test_errors)\n",
    "    best_lambda = lambdas[best_i]\n",
    "\n",
    "    # Compute the dot products we need\n",
    "    size, means_X, mean_Y, YT_Y, YT_X, COV_X  = statistics.map(lambda x : x[1]).reduce(reduce_statistics)\n",
    "    \n",
    "    # Reshape our vectors\n",
    "    means_X = means_X.reshape(-1, 1)\n",
    "    YT_X = YT_X.reshape(-1, 1)\n",
    "    \n",
    "    p = COV_X.shape[0]\n",
    "    XT_X = size * (COV_X + means_X.dot(means_X.T))\n",
    "    D_inv = np.diag([1 / np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "    D = np.diag([np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "    \n",
    "    def beta_objective(beta):\n",
    "        pen_term = 0\n",
    "        if penalizer == \"Ridge\":\n",
    "            pen_term = lmbda + np.linalg.norm(beta**2)\n",
    "        \n",
    "        # Change of variables \n",
    "        beta = D.dot(beta)\n",
    "        \n",
    "        # Compute simplified objective function for beta\n",
    "        linear_term = -(YT_X - size * mean_Y * means_X).T.dot(D_inv).dot(beta)\n",
    "        quadratic_term = 1 /2 * beta.dot(D_inv).dot(XT_X - size * means_X.dot(means_X.T)).dot(D_inv).dot(beta)\n",
    "        return linear_term + quadratic_term + pen_term\n",
    "    \n",
    "    # Compute final alpha, beta\n",
    "    beta = minimize(beta_objective, np.zeros(p), method=\"CG\").x\n",
    "    alpha_hat = mean_Y\n",
    "    alpha = alpha_hat - means_X.T.dot(beta)\n",
    "    \n",
    "    return (alpha, beta, best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques tests\n",
    "\n",
    "Nous commençons par tester notre algorithme pour $n = 500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Design random feature matrix\n",
    "n, p = 500, 100\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "\n",
    "# Set coefficients\n",
    "idx = np.arange(p)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[40:] = 0.\n",
    "    \n",
    "y = X.dot(coefs)\n",
    "\n",
    "Data_array = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data_rdd = sc.parallelize(Data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.81162156e-05] 0.004597685090530298 0.01\n"
     ]
    }
   ],
   "source": [
    "alpha, beta, lmbda = PenalizedLR_MR(Data=Data_rdd, k=5, lambdas=np.logspace(-2, 2, 5), penalizer=\"Ridge\")\n",
    "\n",
    "print(\"Estimated intercept : {}\".format(alpha))\n",
    "print(\"Euclidian distance between true coefficients and estimated coefficients : {}\".format(np.linalg.norm(beta - coefs, ord=2)))\n",
    "print(\"Best penalization parameter found by CV : {}\".format(lmbda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre algorithme parvient à estimer correctement le terme de biais et les coefficients. Nous le comparons avec ce qu'obtiens la régression Ridge implémentée dans scikit-learn avec CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008422742420145918 0.32487655733699944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = []\n",
    "lambdas = np.logspace(-2, 2, 5)\n",
    "for lmbda in lambdas:\n",
    "    clf = Ridge(lmbda, fit_intercept=True)\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "best_i = np.argmin(scores)\n",
    "best_lambda = lambdas[best_i]\n",
    "clf = Ridge(best_lambda, fit_intercept=True)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('Estimated intercept : {}'.format(clf.intercept_))\n",
    "print(\"Euclidian distance between true coefficients and estimated coefficients : {}\".format(np.linalg.norm(clf.coef_ - coefs, ord=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 14.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit PenalizedLR_MR(Data=Data_rdd, k=5, lambdas=np.logspace(-2, 2, 5), penalizer=\"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 63.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "scores = []\n",
    "lambdas = np.logspace(-2, 2, 5)\n",
    "for lmbda in lambdas:\n",
    "    clf = Ridge(lmbda, fit_intercept=True)\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "best_i = np.argmin(scores)\n",
    "best_lambda = lambdas[best_i]\n",
    "clf = Ridge(best_lambda, fit_intercept=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nous essayons maintenant un problème plus grand avec $n = 10000$. Nous comparons notre algorithme avec celui de Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = 10000, 100\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "\n",
    "idx = np.arange(p)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[40:] = 0.\n",
    "    \n",
    "y = X.dot(coefs)\n",
    "\n",
    "Data_array = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data_rdd = sc.parallelize(Data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 20.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit PenalizedLR_MR(Data=Data_rdd, k=5, lambdas=np.logspace(-2, 2, 5), penalizer=\"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 603 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "scores = []\n",
    "lambdas = np.logspace(-2, 2, 5)\n",
    "for lmbda in lambdas:\n",
    "    clf = Ridge(lmbda, fit_intercept=True)\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "best_i = np.argmin(scores)\n",
    "best_lambda = lambdas[best_i]\n",
    "clf = Ridge(best_lambda, fit_intercept=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement nous testons notre algorithme avec $n = 100000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = 100000, 100\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "\n",
    "\n",
    "idx = np.arange(p)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[40:] = 0.\n",
    "    \n",
    "y = X.dot(coefs)\n",
    "\n",
    "Data_array = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data_rdd = sc.parallelize(Data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 24.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit PenalizedLR_MR(Data=Data_rdd, k=5, lambdas=np.logspace(-2, 2, 5), penalizer=\"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 5.54 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "scores = []\n",
    "lambdas = np.logspace(-2, 2, 5)\n",
    "for lmbda in lambdas:\n",
    "    clf = Ridge(lmbda, fit_intercept=True)\n",
    "    score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "best_i = np.argmin(scores)\n",
    "best_lambda = lambdas[best_i]\n",
    "clf = Ridge(best_lambda, fit_intercept=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement nous n'avons pu tester notre algorithme sur un cluster, la comparaison entre le Ridge de Scikit-learn et notre implémentation MapReduce n'est pas juste pour cette dernière, étant donné que MapReduce sert à traiter de grands volumes de données en distribuant les calculs sur un cluster.\n",
    "\n",
    "Néanmoins, nous remarquons que pour $n$ petit, la régression Ridge de scikit-learn est beaucoup plus rapide que notre implémentation, ce qui est normal vu qu'elle est spécialement optimisée pour Python (probablement codée en cython même).\n",
    "\n",
    "Cependant lorsque $n$ augmente l'écart diminue sensiblement passant de au moins 20 fois rapide pour $n=500$ à moins de 5 fois plus rapide pour $n=100000$. \n",
    "\n",
    "Ceci laisse supposer que notre méthode pourrait battre la régression Ridge de scikit-learn en temps d'éxécution si elle était executée sur un grand nombre de machines et que les fonctions hors MapReduce étaient codées en cython par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
