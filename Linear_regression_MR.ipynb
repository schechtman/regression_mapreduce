{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "article : https://arxiv.org/pdf/1307.0048.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée du projet est d'implementer un algorithme mapreduce sur la régression linéaire pénalisée, quand $X \\in \\mathbb{R}^{n \\times p}$ avec $p << n$.\n",
    "\n",
    "Cela correspond à un type de problème ou le nombre de features $p$ (les caractéristiques d'un individu ou un produit) est assez petit et il est envisageable de les stocker en mémoire, alors que la taille du dataset $n$ et très grande et on voudrait faire du calcul distributé dessus\n",
    "\n",
    "L'idée de l'agorithme est alors d'exprimer la quantité à minimiser en fonction des matrices ou des vecteurs dont la dimension est une fonction de $p$ ($p\\times p$ ou $p$ en fait). Et calculer ces quantitées à partir de $X \\in \\mathbb{R}^{n \\times p}$ en faisant une reduction sur $n$ (qui est la taille de notre dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "p = 10\n",
    "n = 100\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), 100)\n",
    "\n",
    "if False:\n",
    "    idx = np.arange(p)\n",
    "    coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "    coefs[20:] = 0.\n",
    "    \n",
    "coefs = np.ones(p)\n",
    "\n",
    "y = X.dot(coefs)\n",
    "\n",
    "Data = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data = sc.parallelize(Data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1\n",
    "\n",
    "En notant $X_c \\in \\mathbb{R}^{n \\times p }$ la matrice centrée réduite de $X$. On a que minimiser:\n",
    "$$||Y - \\alpha \\mathbb{1} - X \\beta||_2 + p_\\lambda(\\beta)$$\n",
    "\n",
    "Revient au même que minimiser:\n",
    "$$\\begin{align}\n",
    "||Y - \\hat{\\alpha} \\mathbb{1} - X_c \\hat{\\beta}||_2 + p_\\lambda(\\hat{\\beta})\n",
    "\\end{align}$$\n",
    "Avec le changement de variable:\n",
    "\n",
    "$$\\begin{align} \n",
    "\\hat{\\alpha}&= \\alpha + \\left(\\bar{X_1}, \\dots, \\bar{X_p}\\right) \\beta \\\\\n",
    "\\hat{\\beta} &= D \\beta\n",
    "\\end{align}\n",
    " $$\n",
    " \n",
    " avec D la matrice diagonale des déviations standards.\n",
    " \n",
    " \n",
    " Comme maintenant les variables sont centrées, la minimisation en $\\hat{\\alpha}$ donne $\\hat{\\alpha} = \\bar{Y}$, et:\n",
    " $$\\begin{align}\n",
    " \\hat{\\beta}^* &= \\arg\\min_{\\hat{\\beta}} ||Y - \\hat{\\alpha} \\mathbb{1} - X_c \\hat{\\beta}||_2 + p_\\lambda(\\hat{\\beta}) \\\\\n",
    "             &= \\arg\\min_{\\hat{\\beta}} ||(Y - \\hat{\\alpha} \\mathbb{1})||_2^2  + ||X_c \\hat{\\beta}||_2^2 - 2(Y - \\hat{\\alpha} \\mathbb{1})^T X_c \\hat{\\beta}  + p_\\lambda(\\hat{\\beta}) \\\\\n",
    "             &= \\arg\\min_{\\hat{\\beta}} ||X_c \\hat{\\beta}||_2^2 - 2(Y - \\hat{\\alpha} \\mathbb{1})^T X_c \\hat{\\beta}  + p_\\lambda(\\hat{\\beta}) \\\\\n",
    "             &= \\arg\\min_{\\hat{\\beta}} \\hat{\\beta}^TX_c^T X_c \\hat{\\beta} - 2Y^T X_c\\hat{\\beta}  + p_\\lambda(\\hat{\\beta})\n",
    " \\end{align}$$\n",
    " \n",
    " $X_c^TX_c, Y^TX_c$ sont une matrice de taille $p \\times p$ et un vecteur de taille $p$. On peut donc par hypothèse les stocker en mémoire et résoudre ce problème par une des méthodes d'optimisation classiques (coordinate descent par exemple).\n",
    " \n",
    " Les quantités qu'on doit calculer sont $X_c^TX_c$ qui est la matrice de correlation de $X$. $Y^TX_c$ et $(\\bar{X_1}, \\dots, \\bar{X_p})$ (pour faire le changement de variables inverse).\n",
    " \n",
    " # En fait pas sûr de mon truc car quand on fait du cross validation centrée sur k-1 partition  с'est pas pareil que de centrer sur tout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize \n",
    "def PenalizedLR_MR(Data, k, lambdas, penalizer=\"ridge\"):\n",
    "    \"\"\"\n",
    "    Data: an RDD each rows of which is a tuple (x, y)\n",
    "    k: number of partitions for splitting\n",
    "    lambdas: list of lambdas to test on\n",
    "    penalizer: penalization term (only \"ridge\" is avaialble for now)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #calculate means, variance,  standardize X\n",
    "    def reduce_mean(row1, row2):\n",
    "        s1 = row1[0]\n",
    "        s2 = row2[0]\n",
    "        \n",
    "        return (s1 + s2, s1 / (s1 + s2) * row1[1] + s2 / (s1 + s2) * row2[1])\n",
    "    \n",
    "    #vector of means of length p    \n",
    "    means_X = np.array(Data.map(lambda row: (1, row[0])).reduce(reduce_mean)[1]) \n",
    "    mean_Y = Data.map(lambda row: (1, row[1])).reduce(reduce_mean)[1]\n",
    "    \n",
    "    p = len(means_X)\n",
    "    #center X\n",
    "    Data.map(lambda row: (np.arrray([row[0][i] - means_X[i] for i in range(p)]), row[1]))\n",
    "    \n",
    "    # vector of variance of X of length p (using the fact that now X is centered)\n",
    "    vars_X = np.array(Data.map(lambda row: (1, row[0]**2)).reduce(reduce_mean)[1])\n",
    "    \n",
    "    #standardize X\n",
    "    Data.map(lambda row: (np.array([row[1][i] / vars_X[i] for i in range(p)]), row[1]))\n",
    "    \n",
    "        \n",
    "    def map_statistics(row):\n",
    "        # calculate statistics for one row [size, mean(x), mean(y), Y^TY, y * x, cov(x)]\n",
    "        x = row[0]\n",
    "        y = row[1]\n",
    "        return (np.random.randint(k), [1, x, y, y**2, y * x, np.zeros((len(row[0]), len(row[0])))])\n",
    "\n",
    "    statistics = Data.map(map_statistics)\n",
    "\n",
    "    def reduce_statistics(row1, row2):\n",
    "        #returns [size, mean(X), mean(Y), Y^TY, Y^TY, Cov(X)]\n",
    "        s_1 = row1[0]\n",
    "        s_2 = row2[0]\n",
    "        mean_x = s_1 / (s_1 + s_2) * row1[1] + s_2 / (s_1 + s_2) * row2[1]\n",
    "        mean_y = s_1 / (s_1 + s_2) * row1[2] + s_2 / (s_1 + s_2) * row2[2]\n",
    "        cov = s_1 / (s_1 + s_2) * row1[5] + s_2 / (s_1 + s_2) * row2[5] + s_1 * s_2 / (\n",
    "            s_1 + s_2)**2 * (row1[1] - row2[1]).T.dot(row1[1] - row2[1])\n",
    "        emit = [s_1 + s_2, mean_x, mean_y, row1[3] +\n",
    "                row2[3], row1[4] + row2[4], cov]\n",
    "        return emit\n",
    "\n",
    "    statistics = statistics.reduceByKey(reduce_statistics)\n",
    "\n",
    "    # Cross validation\n",
    "    test_errors = []\n",
    "    for lmbda in lambdas:\n",
    "        error = 0\n",
    "        for i in range(k):\n",
    "            #do the split\n",
    "            statistics_train = statistics.filter(lambda row: row[0] != i)\n",
    "            statistics_train = statistics_train.reduce(reduce_statistics)[1]\n",
    "            \n",
    "            statistics_test = statistics.filter(lambda row: row[0] == i).collect()[0][1]\n",
    "            #calculate statistics for our train dataset\n",
    "            size = statistics_train[0]\n",
    "            means_X = statistics_train[1]\n",
    "            mean_Y = np.array(statistics_train[2])\n",
    "            YT_Y = statistics_train[3]\n",
    "            YT_X = np.array(statistics_train[4])\n",
    "            COV_X = np.array(statistics_train[5])\n",
    "            XT_X = COV_X + (size - 1) * means_X.T.dot(means_X)\n",
    "            \n",
    "            D_inv = np.diag([1 / np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "            \n",
    "            #calculate statistics foro our test dataset:\n",
    "            size_test = statistics_test[0]\n",
    "            means_X_test = statistics_test[1]\n",
    "            mean_Y_test = statistics_test[2]\n",
    "            YT_Y_test = statistics_test[3]\n",
    "            YT_X_test = statistics_test[4]\n",
    "            COV_X_test = statistics_test[5]\n",
    "            \n",
    "            XT_X_test = COV_X_test + (size_test - 1) * means_X_test.T.dot(means_X_test)\n",
    "            D_inv_test = np.diag([1 / np.sqrt(COV_X_test[i,i]) for i in range(p)])\n",
    "            \n",
    "            def beta_objective(beta):\n",
    "                #the simplified objective function for beta\n",
    "                linear_term = (YT_X - size * mean_Y * means_X).dot(D_inv).dot(beta)\n",
    "                quadratic_term = 1 /2 * beta.dot(D_inv).dot(XT_X - size * means_X.T.dot(means_X)).dot(D_inv).dot(beta)\n",
    "                if penalizer==\"ridge\":\n",
    "                    penalization_term = 1 / 2 * np.linalg.norm(beta, ord=2)\n",
    "                else:\n",
    "                    penalization_term = 0\n",
    "                return linear_term + quadratic_term + lmbda * penalization_term\n",
    "            \n",
    "                \n",
    "            def beta_objective_gradient(beta):\n",
    "                #calculate gradients of each term\n",
    "                if penalizer==\"ridge\":\n",
    "                    penalization_term = beta\n",
    "                else:\n",
    "                    penalization_term = 0\n",
    "                linear_term = (YT_X - size * mean_Y * means_X).dot(D_inv)\n",
    "                quadratic_term = beta.dot(D_inv).dot(XT_X - size * means_X.T.dot(means_X)).dot(D_inv)\n",
    "                \n",
    "                return linear_term + quadratic_term + lmbda * penalization_term\n",
    "            \n",
    "            alpha_hat = mean_Y\n",
    "            beta_hat = minimize(beta_objective, np.zeros(p), method=\"CG\", jac=beta_objective_gradient).x\n",
    "            \n",
    "            def test_error(alpha, beta):\n",
    "                quadratic_term = YT_Y_test + size_test * alpha**2 + beta.dot(D_inv_test).dot(XT_X_test - size_test * means_X_test.T.dot(means_X_test)).dot(D_inv).dot(beta)\n",
    "                double_term = -2 * alpha * mean_Y_test -2 * (YT_X_test - size_test * mean_Y * means_X).dot(D_inv).dot(beta)\n",
    "                return quadratic_term + double_term\n",
    "            error += test_error(alpha_hat, beta_hat)\n",
    "        test_errors.append(error)\n",
    "        \n",
    "    best_i = np.argmin(test_errors)\n",
    "    best_lambda = lambdas[best_i]\n",
    "\n",
    "    #calculate statistics\n",
    "    statistics = statistics.map(lambda row: row[1]).reduce(reduce_statistics)\n",
    "    \n",
    "    size = statistics[0]\n",
    "    means_X = statistics[1]\n",
    "    mean_Y = statistics[2]\n",
    "    YT_Y = statistics[3]\n",
    "    YT_X = statistics[4]\n",
    "    COV_X = statistics[5]\n",
    "\n",
    "    XT_X = COV_X + (size - 1) * means_X.T.dot(means_X)\n",
    "    \n",
    "    D_inv = np.diag([1 / np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "    \n",
    "    def beta_objective(beta):\n",
    "        #the simplified objective function for beta\n",
    "        linear_term = (YT_X - size * mean_Y * means_X).dot(D_inv).dot(beta)\n",
    "        quadratic_term = 1 /2 * beta.dot(D_inv).dot(XT_X - size * means_X.T.dot(means_X)).dot(D_inv).dot(beta)\n",
    "        if penalizer==\"ridge\":\n",
    "            penalization_term = 1 / 2 * np.linalg.norm(beta, ord=2)\n",
    "        else:\n",
    "            penalization_term = 0\n",
    "        return linear_term + quadratic_term + lmbda * penalization_term\n",
    "\n",
    "\n",
    "    def beta_objective_gradient(beta):\n",
    "        #calculate gradients of each term\n",
    "        if penalizer==\"ridge\":\n",
    "            penalization_term = beta\n",
    "        else:\n",
    "            penalization_term = 0\n",
    "        linear_term = (YT_X - size * mean_Y * means_X).dot(D_inv)\n",
    "        quadratic_term = beta.dot(D_inv).dot(XT_X - size * means_X.T.dot(means_X)).dot(D_inv)\n",
    "\n",
    "        return linear_term + quadratic_term + lmbda * penalization_term\n",
    "    alpha_hat = mean_Y\n",
    "    beta_hat = minimize(beta_objective, np.zeros(p), method=\"CG\", jac=beta_objective_gradient).x\n",
    "    \n",
    "    print(alpha_hat, \"\\n\")\n",
    "    print(means_X)\n",
    "    print(beta_hat)  \n",
    "    print(beta_objective(beta_hat))\n",
    "    beta = D_inv.dot(beta_hat)\n",
    "    alpha = alpha_hat - means_X.T.dot(beta)\n",
    "    \n",
    "    return (alpha, beta, best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.048164441201812055 \n",
      "\n",
      "[ 2.03101752e-01 -1.98978099e-02  1.81806798e-02 -1.21368099e-01\n",
      "  6.94640114e-02  2.26584509e-03 -1.29893643e-04 -6.94826179e-02\n",
      "  6.17316673e-02 -1.92029976e-01]\n",
      "[ 1.99131038e+17  4.08601611e+17 -1.74548145e+16 -1.93643184e+17\n",
      " -3.46814935e+17  7.16903245e+16 -5.81265492e+17  1.75790948e+17\n",
      "  1.64903021e+17  1.19061479e+17]\n",
      "-6.259820860377334e+18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-2191945045476063.0,\n",
       " array([ 6.46922703e+16,  1.32743575e+17, -5.67059557e+15, -6.29094154e+16,\n",
       "        -1.12670761e+17,  2.32902410e+16, -1.88837384e+17,  5.71097085e+16,\n",
       "         5.35725166e+16,  3.86798436e+16]),\n",
       " 0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PenalizedLR_MR(Data=Data, k=2, lambdas=[0], penalizer=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1]), 1), (array([2]), 2), (array([3]), 3), (array([4]), 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = sc.parallelize([(np.array([i]), i) for i in range(1, 10)])\n",
    "test_data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [9, array([[5.]]), 5.0, 285, array([[285]]), array([[6.66666667]])])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat = PenalizedLR_MR(test_data, 1, [1])\n",
    "stat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6.66666667)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(np.arange(1,10), bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
