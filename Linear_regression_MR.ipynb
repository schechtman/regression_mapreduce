{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "article : https://arxiv.org/pdf/1307.0048.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée du projet est d'implementer un algorithme mapreduce sur la régression linéaire pénalisée, quand $X \\in \\mathbb{R}^{n \\times p}$ avec $p << n$.\n",
    "\n",
    "Cela correspond à un type de problème ou le nombre de features $p$ (les caractéristiques d'un individu ou un produit) est assez petit et il est envisageable de les stocker en mémoire, alors que la taille du dataset $n$ et très grande et on voudrait faire du calcul distributé dessus\n",
    "\n",
    "L'idée de l'agorithme est alors d'exprimer la quantité à minimiser en fonction des matrices ou des vecteurs dont la dimension est une fonction de $p$ ($p\\times p$ ou $p$ en fait). Et calculer ces quantitées à partir de $X \\in \\mathbb{R}^{n \\times p}$ en faisant une reduction sur $n$ (qui est la taille de notre dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "p = 100\n",
    "n = 10000\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "\n",
    "if True:\n",
    "    idx = np.arange(p)\n",
    "    coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "    coefs[40:] = 0.\n",
    "    \n",
    "y = X.dot(coefs)\n",
    "\n",
    "Data_array = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data_rdd = sc.parallelize(Data_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "p = 10\n",
    "n = 100\n",
    "cov = toeplitz(0.5 ** np.arange(p))\n",
    "X = multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "\n",
    "coefs = np.ones(p)\n",
    "\n",
    "y = X.dot(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1\n",
    "\n",
    "En notant $X_c \\in \\mathbb{R}^{n \\times p }$ la matrice centrée réduite de $X$. On a :\n",
    "$$X_c = (X - \\mathbb{1} (\\bar{X_1}, \\dots, \\bar{X_p}))D^{-1}$$\n",
    "\n",
    "et\n",
    "$$\\begin{align}\n",
    "&||Y - \\alpha \\mathbb{1} - X \\beta||_2 + p_\\lambda(\\beta) \\\\\n",
    "= & ||Y - \\alpha \\mathbb{1} - (X_cD + \\mathbb{1} (\\bar{X_1}, \\dots, \\bar{X_p})) \\beta||_2 + p_\\lambda(\\beta) \\\\\n",
    "= & ||Y - (\\alpha + (\\bar{X_1}, \\dots, \\bar{X_p})) \\beta) \\mathbb{1} - X_c D \\beta ||_2 + p_\\lambda(\\beta)\n",
    "\\end{align}$$\n",
    "\n",
    "On a donc que minimiser:\n",
    "$$||Y - \\alpha \\mathbb{1} - X \\beta||_2 + p_\\lambda(\\beta) $$\n",
    "revient à minimiser:\n",
    "\n",
    "$$\\begin{align}\n",
    "||Y - \\hat{\\alpha} \\mathbb{1} - X_c \\hat{\\beta}||_2 + p_\\lambda(\\hat{\\beta})\n",
    "\\end{align}$$\n",
    "Avec le changement de variable:\n",
    "\n",
    "$$\\begin{align} \n",
    "\\hat{\\alpha}&= \\alpha + \\left(\\bar{X_1}, \\dots, \\bar{X_p}\\right) \\beta \\\\\n",
    "\\hat{\\beta} &= D \\beta\n",
    "\\end{align}\n",
    " $$\n",
    " \n",
    " avec D la matrice diagonale des déviations standards.\n",
    " \n",
    " \n",
    " Comme maintenant les variables sont centrées, la minimisation en $\\hat{\\alpha}$ donne $\\hat{\\alpha} = \\bar{Y}$, et:\n",
    " $$\\begin{align}\n",
    " \\hat{\\beta}^* &= \\arg\\min_{\\hat{\\beta}} ||Y - \\hat{\\alpha} \\mathbb{1} - X_c \\hat{\\beta}||_2 + p_\\lambda(\\hat{\\beta}) \\\\\n",
    "             &= \\arg\\min_{\\hat{\\beta}} ||(Y - \\hat{\\alpha} \\mathbb{1})||_2^2  + ||X_c \\hat{\\beta}||_2^2 - 2(Y - \\hat{\\alpha} \\mathbb{1})^T X_c \\hat{\\beta}  + p_\\lambda(\\hat{\\beta}) \\\\\n",
    "             &= \\arg\\min_{\\hat{\\beta}} ||X_c \\hat{\\beta}||_2^2 - 2(Y - \\hat{\\alpha} \\mathbb{1})^T X_c \\hat{\\beta}  + p_\\lambda(\\hat{\\beta}) \\\\\n",
    "             &= \\arg\\min_{\\hat{\\beta}} \\hat{\\beta}^TX_c^T X_c \\hat{\\beta} - 2Y^T X_c\\hat{\\beta}  + p_\\lambda(\\hat{\\beta})\n",
    " \\end{align}$$\n",
    " \n",
    " $X_c^TX_c, Y^TX_c$ sont une matrice de taille $p \\times p$ et un vecteur de taille $p$. On peut donc par hypothèse les stocker en mémoire et résoudre ce problème par une des méthodes d'optimisation classiques (coordinate descent par exemple).\n",
    " \n",
    " Les quantités qu'on doit calculer sont $X_c^TX_c$ qui est la matrice de correlation de $X$. $Y^TX_c$ et $(\\bar{X_1}, \\dots, \\bar{X_p})$ (pour faire le changement de variables inverse).\n",
    " \n",
    " # En fait pas sûr de mon truc car quand on fait du cross validation centrée sur k-1 partition  с'est pas pareil que de centrer sur tout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, p = 100, 50\n",
    "X = np.random.randint(20, size=(n,p))\n",
    "coeffs = np.ones(p)\n",
    "y = X.dot(coeffs)\n",
    "Data_array = [(X[i], y[i]) for i in range(n)]\n",
    "\n",
    "Data_rdd = sc.parallelize(Data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha, beta \n",
      " [-1.65064462e-10] \n",
      "  [ 5.75388372e-09  9.04837409e-01 -1.81901021e-09  7.40818214e-01\n",
      " -3.51445617e-09  6.06530669e-01 -5.25056089e-09  4.96585306e-01\n",
      "  9.91301333e-09  4.06569657e-01  3.39766618e-09  3.32871085e-01\n",
      " -6.24473255e-09  2.72531797e-01 -9.30253687e-09  2.23130166e-01\n",
      " -7.65121052e-09  1.82683513e-01 -6.04026846e-09  1.49568617e-01\n",
      " -2.54119166e-09  1.22456434e-01  1.37212654e-09  1.00258837e-01\n",
      " -3.23362769e-09  8.20849966e-02  7.24651633e-09  6.72055213e-02\n",
      " -6.74280835e-09  5.50232244e-02  4.82137097e-09  4.50492016e-02\n",
      "  5.94053877e-09  3.68831817e-02 -3.35281421e-09  3.01973947e-02\n",
      "  5.82446470e-09  2.47235294e-02 -7.69020203e-09  2.02419170e-02\n",
      " -7.63466638e-09 -1.01384692e-08 -3.89536463e-09  3.07001461e-09\n",
      " -2.64306746e-09 -1.33516612e-08 -1.54638222e-09 -8.76548470e-09\n",
      "  1.84372819e-09 -4.80020189e-09 -3.08247523e-09  4.75321205e-09\n",
      " -4.98373406e-09  3.35765596e-09  5.93557678e-09  2.00095395e-08\n",
      " -4.10275571e-09 -1.55033461e-09 -1.04717361e-08 -1.04076168e-08\n",
      "  3.62640651e-09 -1.75058834e-09  4.59437431e-09  2.93202848e-09\n",
      " -3.72583753e-10 -3.46887130e-10 -1.66583791e-09  3.87075104e-09\n",
      " -2.64176105e-09  6.78047767e-09  1.80971222e-09 -6.04174442e-09\n",
      " -9.21733786e-09 -8.83777003e-09  2.78488948e-09 -2.76766909e-09\n",
      "  2.16021033e-09 -4.79097890e-09 -5.59851742e-09  2.89171496e-09\n",
      " -1.63573681e-08  1.37552985e-08 -1.68845760e-09 -6.92646977e-09\n",
      " -7.43649134e-09  7.35769947e-09 -4.77679562e-09  2.90996784e-09\n",
      " -1.49604624e-08  7.31993825e-09 -5.95361042e-09  2.59438982e-09\n",
      "  1.28188746e-09 -5.14364958e-09 -6.82546383e-09 -2.59050577e-09\n",
      " -2.45710979e-09 -7.95086032e-10 -4.08974348e-09  1.11079867e-09] \n",
      "\n",
      "0.0 [-1.12947066e-02  9.15931077e-01 -3.00448017e-03  7.46383027e-01\n",
      " -6.76658905e-04  6.17054117e-01  3.26898455e-02  4.98095160e-01\n",
      "  9.58704769e-03  3.79938078e-01  1.11349924e-02  3.24316070e-01\n",
      " -6.59381692e-03  2.69023289e-01 -3.61727605e-03  2.25505135e-01\n",
      " -1.58433050e-02  1.80034730e-01 -3.91371769e-04  1.74857099e-01\n",
      "  1.28477539e-02  1.17956347e-01  4.13967129e-03  9.04268868e-02\n",
      "  1.54167616e-02  9.25156851e-02  1.79384453e-02  6.68106682e-02\n",
      "  3.05367836e-03  4.86792262e-02  6.63536043e-03  4.32190489e-02\n",
      "  1.58299298e-03  2.72402647e-02  9.88091118e-03  4.03470475e-02\n",
      " -6.95968990e-07  1.21071282e-02  5.50084855e-03  1.93061393e-02\n",
      "  2.11850979e-02 -2.00967662e-02  5.12260529e-03  6.27330147e-03\n",
      "  2.53238239e-02 -1.62037227e-02  1.53593222e-02  7.33583434e-03\n",
      " -5.01599593e-03 -2.10221496e-03  5.03712691e-03  1.66098316e-02\n",
      "  8.29312143e-03 -6.64101484e-03 -1.07923809e-02 -1.67908884e-02\n",
      " -8.92842386e-03  4.42945919e-03 -6.37196160e-03  7.21353920e-03\n",
      " -6.99875225e-05  1.17162521e-02 -7.91612948e-03  3.74032011e-03\n",
      "  1.27303350e-02 -1.44299680e-02 -1.14207155e-02 -3.36750184e-03\n",
      "  1.36119406e-02 -5.79320824e-03 -2.83432175e-03 -8.80698786e-03\n",
      " -6.07009113e-04 -1.03985951e-02  1.84692986e-03 -8.95721229e-04\n",
      "  8.51103827e-03 -5.89363069e-03  1.54015220e-03  8.99347717e-03\n",
      " -5.28698313e-03  3.40976969e-03 -1.65853662e-02  1.21676562e-02\n",
      "  2.88628234e-03 -9.89692722e-04 -1.28466468e-02 -1.85975939e-03\n",
      " -6.07472686e-03 -1.38591126e-03  8.61011526e-03  8.13891673e-03\n",
      " -7.47786845e-03  5.46962851e-03 -2.28530844e-03 -6.56249924e-04\n",
      "  4.64076926e-03  3.26995446e-03 -9.64456429e-04 -1.58541646e-03]\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "def LR_MR(Data):\n",
    "    def reduce_mean(row1, row2):\n",
    "        s1 = row1[0]\n",
    "        s2 = row2[0]\n",
    "        \n",
    "        return (s1 + s2, s1 / (s1 + s2) * row1[1] + s2 / (s1 + s2) * row2[1])\n",
    "    \n",
    "    def map_statistics(row):\n",
    "        # calculate statistics for one row [size, mean(x), mean(y), Y^TY, y * x, cov(x)]\n",
    "        x = row[0]\n",
    "        y = row[1]\n",
    "        return [1, x, y, y**2, y * x, np.zeros((len(row[0]), len(row[0])))]\n",
    "\n",
    "    statistics = Data.map(map_statistics)\n",
    "\n",
    "    def reduce_statistics(row1, row2):\n",
    "        #combined with map_statistics returns [size, mean(X), mean(Y), Y^TY, Y^TX, Cov(X)]\n",
    "        s_1 = row1[0]\n",
    "        s_2 = row2[0]\n",
    "        mean_x = s_1 / (s_1 + s_2) * row1[1] + s_2 / (s_1 + s_2) * row2[1]\n",
    "        mean_y = s_1 / (s_1 + s_2) * row1[2] + s_2 / (s_1 + s_2) * row2[2]\n",
    "        \n",
    "        mean_substraction = (row1[1] - row2[1]).reshape((1, -1))\n",
    "        cov = s_1 / (s_1 + s_2) * row1[5] + s_2 / (s_1 + s_2) * row2[5] + s_1 * s_2 / (\n",
    "            s_1 + s_2)**2 * (mean_substraction).T.dot(mean_substraction)\n",
    "        emit = [s_1 + s_2, mean_x, mean_y, row1[3] +\n",
    "                row2[3], row1[4] + row2[4], cov]\n",
    "        return emit\n",
    "    \n",
    "    statistics = statistics.reduce(reduce_statistics)\n",
    "    \n",
    "    size = statistics[0]\n",
    "    means_X = statistics[1].reshape(-1, 1)\n",
    "    mean_Y = statistics[2]\n",
    "    YT_Y = statistics[3]\n",
    "    YT_X = statistics[4].reshape(-1, 1)\n",
    "    COV_X = statistics[5] \n",
    "    p = COV_X.shape[0]\n",
    "    XT_X = size * (COV_X + means_X.dot(means_X.T))\n",
    "    D_inv = np.diag([1 / np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "    D = np.diag([np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "    \n",
    "    if False:\n",
    "        #c'était pour vérifier que les valeurs sont bien calculées mais a priori c'est bon \n",
    "        print(\"COV_X \\n {}\".format(COV_X))\n",
    "        print(size)\n",
    "        print(\"XT_X \\n {} \\n\".format(XT_X))\n",
    "        print(\"means X \\n {} \\n\".format(means_X))\n",
    "    \n",
    "    def beta_objective(beta):\n",
    "        beta = D.dot(beta) #changement de variable en beta_hat\n",
    "        #the simplified objective function for beta\n",
    "        linear_term = -(YT_X - size * mean_Y * means_X).T.dot(D_inv).dot(beta)\n",
    "        quadratic_term = 1 /2 * beta.dot(D_inv).dot(XT_X - size * means_X.dot(means_X.T)).dot(D_inv).dot(beta)\n",
    "        return linear_term + quadratic_term\n",
    "\n",
    "    def beta_objective_gradient(beta):\n",
    "        #calculate gradients of each term\n",
    "        linear_term = -(YT_X - size * mean_Y * means_X).T.dot(D_inv)\n",
    "        quadratic_term = beta.dot(D_inv).dot(XT_X - size * means_X.dot(means_X.T)).dot(D_inv)\n",
    "\n",
    "        return np.ravel(linear_term + quadratic_term)\n",
    "    \n",
    "    #changement de variable inverse:\n",
    "    alpha_hat = mean_Y\n",
    "    beta = minimize(beta_objective, np.zeros(p), method=\"CG\").x \n",
    "    alpha = alpha_hat - means_X.T.dot(beta)\n",
    "    print(\"alpha, beta \\n {} \\n  {} \\n\".format(alpha, beta))\n",
    "    return (alpha, beta)\n",
    "\n",
    "alpha1, beta1 = LR_MR(Data=Data_rdd)\n",
    "\n",
    "\n",
    "# pour comparer avec la vrai valeur\n",
    "\n",
    "def objective(beta_alpha):\n",
    "    alpha = beta_alpha[0]\n",
    "    beta = beta_alpha[1:]\n",
    "    return np.linalg.norm(y - X.dot(beta) - np.ones(y.shape[0]), ord=2)\n",
    "\n",
    "\n",
    "beta_alpha = minimize(objective, np.zeros(p + 1)).x\n",
    "alpha = beta_alpha[0]\n",
    "beta = beta_alpha[1:]\n",
    "print(alpha, beta)\n",
    "\n",
    "if True:\n",
    "    COV_X = np.cov(X, rowvar=False, bias=True)\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    means_X = np.mean(X, axis=0).reshape(1, -1)\n",
    "    D_inv = np.diag([COV_X[i, i] for i in range(p)])\n",
    "    mean_Y = np.mean(y)\n",
    "    if False:\n",
    "        print(\"COV_X \\n {} \\n\".format(COV_X))\n",
    "        print(\" X.T.X \\n {} \\n\".format(X.T.dot(X)))\n",
    "        means_X = np.mean(X, axis=0).reshape(1, -1)\n",
    "        print(\"means X \\n {} \\n\".format(means_X))\n",
    "        n = X.shape[0]\n",
    "        print(\"size {}\".format(n))\n",
    "        print(\"cool XTX \\n {} \\n\".format(n * (COV_X + means_X.T.dot(means_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize \n",
    "def PenalizedLR_MR(Data, k, lambdas, penalizer=\"ridge\"):\n",
    "    \"\"\"\n",
    "    Data: an RDD each rows of which is a tuple (x, y)\n",
    "    k: number of partitions for splitting\n",
    "    lambdas: list of lambdas to test on\n",
    "    penalizer: penalization term (only \"ridge\" is avaialble for now)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #calculate means, variance,  standardize X\n",
    "    def reduce_mean(row1, row2):\n",
    "        s1 = row1[0]\n",
    "        s2 = row2[0]\n",
    "        return (s1 + s2, s1 / (s1 + s2) * row1[1] + s2 / (s1 + s2) * row2[1])\n",
    "    \n",
    "    #vector of means of length p    \n",
    "    means_X = np.array(Data.map(lambda row: (1, row[0])).reduce(reduce_mean)[1]) \n",
    "    mean_Y = Data.map(lambda row: (1, row[1])).reduce(reduce_mean)[1]\n",
    "    \n",
    "    p = len(means_X)\n",
    "    #center X\n",
    "    Data.map(lambda row: (np.arrray([row[0][i] - means_X[i] for i in range(p)]), row[1]))\n",
    "    \n",
    "    # vector of variance of X of length p (using the fact that now X is centered)\n",
    "    vars_X = np.array(Data.map(lambda row: (1, row[0]**2)).reduce(reduce_mean)[1])\n",
    "    \n",
    "    #standardize X\n",
    "    Data.map(lambda row: (np.array([row[1][i] / np.sqrt(vars_X[i]) for i in range(p)]), row[1]))\n",
    "    \n",
    "        \n",
    "    def map_statistics(row):\n",
    "        # calculate statistics for one row [size, mean(x), mean(y), Y^TY, y * x, cov(x)]\n",
    "        x = row[0]\n",
    "        y = row[1]\n",
    "        return (np.random.randint(k), [1, x, y, y**2, y * x, np.zeros((len(row[0]), len(row[0])))])\n",
    "\n",
    "    statistics = Data.map(map_statistics)\n",
    "\n",
    "    def reduce_statistics(row1, row2):\n",
    "        #combined with map_statistics returns [size, mean(X), mean(Y), Y^TY, Y^TX, Cov(X)]\n",
    "        s_1 = row1[0]\n",
    "        s_2 = row2[0]\n",
    "        mean_x = s_1 / (s_1 + s_2) * row1[1] + s_2 / (s_1 + s_2) * row2[1]\n",
    "        mean_y = s_1 / (s_1 + s_2) * row1[2] + s_2 / (s_1 + s_2) * row2[2]\n",
    "        cov = s_1 / (s_1 + s_2) * row1[5] + s_2 / (s_1 + s_2) * row2[5] + s_1 * s_2 / (\n",
    "            s_1 + s_2)**2 * (row1[1] - row2[1]).T.dot(row1[1] - row2[1])\n",
    "        emit = [s_1 + s_2, mean_x, mean_y, row1[3] +\n",
    "                row2[3], row1[4] + row2[4], cov]\n",
    "        return emit\n",
    "\n",
    "    statistics = statistics.reduceByKey(reduce_statistics)\n",
    "\n",
    "    # Cross validation\n",
    "    test_errors = []\n",
    "    for lmbda in lambdas:\n",
    "        error = 0\n",
    "        for i in range(k):\n",
    "            #do the split\n",
    "            statistics_train = statistics.filter(lambda row: row[0] != i)\n",
    "            statistics_train = statistics_train.reduce(reduce_statistics)[1]\n",
    "            \n",
    "            statistics_test = statistics.filter(lambda row: row[0] == i).collect()[0][1]\n",
    "            #calculate statistics for our train dataset\n",
    "            size = statistics_train[0]\n",
    "            means_X = statistics_train[1].reshape(-1, 1)\n",
    "            print(means_X.shape)\n",
    "            mean_Y = np.array(statistics_train[2])\n",
    "            YT_Y = statistics_train[3]\n",
    "            YT_X = np.array(statistics_train[4]).reshape(-1, 1)\n",
    "            COV_X = np.array(statistics_train[5])\n",
    "            XT_X = size * ( COV_X + means_X.T.dot(means_X))\n",
    "            \n",
    "            D_inv = np.diag([1 / np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "            \n",
    "            #calculate statistics foro our test dataset:\n",
    "            size_test = statistics_test[0]\n",
    "            means_X_test = statistics_test[1].reshape(-1, 1)\n",
    "            mean_Y_test = statistics_test[2]\n",
    "            YT_Y_test = statistics_test[3]\n",
    "            YT_X_test = statistics_test[4].reshape(-1, 1)\n",
    "            COV_X_test = statistics_test[5]\n",
    "            \n",
    "            \n",
    "            XT_X_test = size_test * (COV_X_test + means_X_test.T.dot(means_X_test))\n",
    "            D_inv_test = np.diag([1 / np.sqrt(COV_X_test[i,i]) for i in range(p)])\n",
    "            \n",
    "            def beta_objective(beta):\n",
    "                #the simplified objective function for beta\n",
    "                linear_term = (YT_X - size * mean_Y * means_X).T.dot(D_inv).dot(beta)\n",
    "                quadratic_term = 1 /2 * beta.dot(D_inv).dot(XT_X - size * means_X.T.dot(means_X)).dot(D_inv).dot(beta)\n",
    "                if penalizer==\"ridge\":\n",
    "                    penalization_term = 1 / 2 * np.linalg.norm(beta, ord=2)\n",
    "                else:\n",
    "                    penalization_term = 0\n",
    "                return linear_term + quadratic_term + lmbda * penalization_term\n",
    "            \n",
    "                \n",
    "            def beta_objective_gradient(beta):\n",
    "                #calculate gradients of each term\n",
    "                if penalizer==\"ridge\":\n",
    "                    penalization_term = beta\n",
    "                else:\n",
    "                    penalization_term = 0\n",
    "                linear_term = (YT_X - size * mean_Y * means_X).T.dot(D_inv)\n",
    "                quadratic_term = beta.dot(D_inv).dot(XT_X - size * means_X.T.dot(means_X)).dot(D_inv)\n",
    "                \n",
    "                return np.ravel(linear_term + quadratic_term + lmbda * penalization_term)\n",
    "            \n",
    "            alpha_hat = mean_Y\n",
    "            print(beta_objective(np.zeros(p)))\n",
    "            print(beta_objective_gradient(np.zeros(p)).shape)\n",
    "            beta_hat = minimize(beta_objective, np.zeros(p), method=\"CG\", jac=beta_objective_gradient).x\n",
    "            \n",
    "            def test_error(alpha, beta):\n",
    "                quadratic_term = YT_Y_test + size_test * alpha**2 + beta.dot(D_inv_test).dot(XT_X_test - size_test * means_X_test.T.dot(means_X_test)).dot(D_inv).dot(beta)\n",
    "                print(mean_Y_test.shape, YT_X_test.shape, size_test, means_X_test.shape)\n",
    "\n",
    "                double_term = -2 * alpha * mean_Y_test -2 * (YT_X_test - size_test * mean_Y_test * means_X_test).T.dot(D_inv).dot(beta)\n",
    "                return quadratic_term + double_term\n",
    "            error += test_error(alpha_hat, beta_hat)\n",
    "        test_errors.append(error)\n",
    "        \n",
    "    best_i = np.argmin(test_errors)\n",
    "    best_lambda = lambdas[best_i]\n",
    "\n",
    "    #calculate statistics\n",
    "    statistics = statistics.map(lambda row: row[1]).reduce(reduce_statistics)\n",
    "    \n",
    "    size = statistics[0]\n",
    "    means_X = statistics[1].reshape(-1, 1)\n",
    "    mean_Y = statistics[2]\n",
    "    YT_Y = statistics[3]\n",
    "    YT_X = statistics[4].reshape(-1, 1)\n",
    "    COV_X = statistics[5]\n",
    "\n",
    "    XT_X = size * (COV_X + means_X.T.dot(means_X))\n",
    "    \n",
    "    D_inv = np.diag([1 / np.sqrt(COV_X[i,i]) for i in range(p)])\n",
    "    \n",
    "    def beta_objective(beta):\n",
    "        #the simplified objective function for beta\n",
    "        linear_term = (YT_X - size * mean_Y * means_X).T.dot(D_inv).dot(beta)\n",
    "        quadratic_term = 1 /2 * beta.dot(D_inv).dot(XT_X - size * means_X.T.dot(means_X)).dot(D_inv).dot(beta)\n",
    "        if penalizer==\"ridge\":\n",
    "            penalization_term = 1 / 2 * np.linalg.norm(beta, ord=2)\n",
    "        else:\n",
    "            penalization_term = 0\n",
    "        return linear_term + quadratic_term + lmbda * penalization_term\n",
    "\n",
    "\n",
    "    def beta_objective_gradient(beta):\n",
    "        #calculate gradients of each term\n",
    "        if penalizer==\"ridge\":\n",
    "            penalization_term = beta\n",
    "        else:\n",
    "            penalization_term = 0\n",
    "        linear_term = (YT_X - size * mean_Y * means_X).T.dot(D_inv)\n",
    "        quadratic_term = beta.dot(D_inv).dot(XT_X - size * means_X.T.dot(means_X)).dot(D_inv)\n",
    "\n",
    "        return np.ravel(linear_term + quadratic_term + lmbda * penalization_term)\n",
    "    alpha_hat = mean_Y\n",
    "    beta_hat = minimize(beta_objective, np.zeros(p), method=\"CG\", jac=beta_objective_gradient).x\n",
    "    \n",
    "\n",
    "    beta = D_inv.dot(beta_hat)\n",
    "    alpha = alpha_hat - means_X.T.dot(beta)\n",
    "    \n",
    "    return (alpha, beta, best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "[0.]\n",
      "(10,)\n",
      "() (10, 1) 68 (10, 1)\n",
      "(10, 1)\n",
      "[0.]\n",
      "(10,)\n",
      "() (10, 1) 32 (10, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([4.26519645e+14]),\n",
       " array([ 1.33572468e+15,  1.14037887e+14, -1.87496611e+15, -8.50901452e+14,\n",
       "         2.70020556e+14, -1.38159084e+15, -1.27280006e+14,  3.45736829e+12,\n",
       "         1.62230867e+15,  8.89189237e+14]),\n",
       " 0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PenalizedLR_MR(Data=Data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
